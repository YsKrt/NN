
+class NN()
	const input_size: int :: 1
	const hidden_size: int :: 100
	const output_size: int :: 1
	var input: []float
	var hidden: []float
	var output: []float
	var weight_ih: [][]float
	var weight_ho: [][]float
	
	var teacher: []float
	
	*func ctor()
		;入力層と隠れ層の[0]には1
		;重みの[0]にはバイアスを用意する
		do me.input :: #[input_size + 1]float
		do me.hidden :: #[hidden_size + 1]float
		do me.output :: #[output_size]float
		do me.teacher :: #[output_size]float
		do me.weight_ih :: #[input_size + 1, hidden_size + 1]float
		do me.weight_ho :: #[hidden_size + 1, output_size]float
		;1の出力
		do me.input[0] :: 1.0
		do me.hidden[0] :: 1.0
		;重みの初期化
		;全て同じ値を使うと更新してもすべて同じ値になってしまい意味がなくなる！！！
		;バイアスも乱数で初期化
		for i(0, input_size)
			for h(0, hidden_size)
				do me.weight_ih[i][h] :: lib@rndFloat(-1.0, 1.0)
				for o(0, output_size - 1)
					do me.weight_ho[h][o] :: lib@rndFloat(-1.0, 1.0)
				end for
			end for
		end for
		
	end func
	;flag
	;0 sigmoid
	;1 ReLU
	;2 tanh
	;3 step
	+func nn(input: []float, teacher: []float, activate: int, learningRate: float, trainF: int): []float
		;入力
		for i(1, input_size)
			do me.input[i] :: input[i - 1]
		end for
		;教師データ
		for o(0, output_size - 1)
			do me.teacher[o] :: teacher[o]
		end for
		;入力層から隠れ層へ
		for h(1, hidden_size)
			var hid: float :: 0.0
			for i(0, input_size)
				do hid :+ me.input[i] * me.weight_ih[i][h]
			end for
			switch(activate)
			case 0
				do me.hidden[h] :: me.sigmoid(hid)
			case 1
				do me.hidden[h] :: me.ReLU(hid)
			case 2
				do me.hidden[h] :: me.tanh(hid)
			case 3
				do me.hidden[h] :: me.step(hid)
			end switch
		end for
		;隠れ層から出力層へ
		for o(0, output_size - 1)
			var out: float :: 0.0
			for h(0, hidden_size)
				do out :+ me.hidden[h] * me.weight_ho[h][o]
			end for
			do me.output[o] :: out
		end for
		;重みの更新
		if(trainF = 1)
			do me.update_w(activate, learningRate)
		end if
		ret me.output
	end func
	
	+func update_w(flag: int, learningRate: float)
		;二乗誤差の勾配(の一部)
		var dE: float
		do dE :: me.d_MSE(me.output, me.teacher)
		for o(0, output_size - 1)
			for h(0, hidden_size)
				;出力層と隠れ層の間の重みとバイアスの更新
				var dw_ho: float :: -learningRate * dE * me.hidden[h]
				do me.weight_ho[h][o] :+ dw_ho
				for i(0, input_size)
					;隠れ層と入力層の間の重みとバイアスの更新
					var dw_ih: float
					switch(flag)
					case 0
						;sigmoid
						do dw_ih :: -learningRate * dE * me.weight_ho[h][o] * (me.input[i] * me.d_sigmoid(me.hidden[h]))
					case 1
						;ReLU
						var _dw_ih: float :: me.hidden[h] > 0.0 ?(-learningRate * dE * me.weight_ho[h][o] * me.input[i], 0.0)
						do dw_ih :: _dw_ih
					case 2
						;tanh
						do dw_ih :: -learningRate * dE * me.weight_ho[h][o] * (me.input[i] * me.d_tanh(me.hidden[h]))
					case 3
						;step
						do dw_ih :: me.hidden[h] > 0.0 ?(-learningRate * dE * me.weight_ho[h][o] * me.input[i], 0.0)
					end switch
					
					do me.weight_ih[i][h] :+ dw_ih
				end for
			end for
		end for
	end func
	
	+func MSE(out: []float, tea: []float): float
		var sum: float
		for o(0, output_size - 1)
			do sum :+ (out[o] - tea[o]) ^ 2.0
		end for
		ret sum / output_size $ float
	end func
	
	+func d_MSE(out: []float, tea: []float): float
		var sum: float
		for o(0, output_size - 1)
			do sum :+ (out[o] - tea[o])
		end for
		ret sum / output_size $ float
	end func
	
	+func tanh(x: float): float
		ret lib@tanh(x)
	end func
	+func d_tanh(f: float): float
		ret 1.0 - f ^ 2.0
	end func
	
	+func step(x: float): float
		ret x > 0.0 ?(1.0, 0.0)
	end func
	
	+func ReLU(x: float): float
		ret x > 0.0 ?(x, 0.0)
	end func
	
	+func sigmoid(x: float): float
		ret 1.0 / (1.0 + lib@e ^ -x)
	end func
	
	+func d_sigmoid(f: float): float
		ret f * (1.0 - f)
	end func
	
	
	+func show()
		;入力
		for i(0, input_size)
			do dbg@print("in[\{i}]:\{me.input[i]}\n")
		end for
		;重み_ih
		for i(0, input_size)
			for h(0, hidden_size)
				do dbg@print("w_ih[\{i}][\{h}]:\{me.weight_ih[i][h]}\n")
			end for
		end for
		;隠れ
		for h(0, hidden_size)
			do dbg@print("hid[\{h}]:\{me.hidden[h]}\n")
		end for
		;重み_ho
		for h(0, hidden_size)
			for o(0, input_size - 1)
				do dbg@print("w_ho[\{h}][\{o}]:\{me.weight_ho[h][o]}\n")
			end for
		end for
		;出力
		for o(0, output_size - 1)
			do dbg@print("out[\{o}]:\{me.output[o]}\n")
		end for
		;教師
		for o(0, output_size - 1)
			do dbg@print("t[\{o}]:\{me.teacher[o]}\n")
		end for
		;誤差関数
		var error: float :: me.MSE(me.output, me.teacher)
		do dbg@print("err:\{error}\n")
	end func
	
end class
